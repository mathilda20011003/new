#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹è·å–æ¨¡å—
ä»å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥è·å–å®Œæ•´çš„æ–‡ç« å†…å®¹
"""

import re
import time
import requests
from typing import Optional, Dict
from bs4 import BeautifulSoup


class WeChatContentFetcher:
    """å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹è·å–å™¨"""
    
    def __init__(self, timeout: int = 30, retry_times: int = 3):
        """
        åˆå§‹åŒ–å†…å®¹è·å–å™¨
        
        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
            retry_times: é‡è¯•æ¬¡æ•°
        """
        self.timeout = timeout
        self.retry_times = retry_times
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    def fetch_article_content(self, url: str) -> Optional[Dict[str, str]]:
        """
        è·å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹
        
        Args:
            url: æ–‡ç« é“¾æ¥
            
        Returns:
            åŒ…å«æ ‡é¢˜å’Œå†…å®¹çš„å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        if not self._is_wechat_article_url(url):
            print(f"âš ï¸ ä¸æ˜¯æœ‰æ•ˆçš„å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥: {url}")
            return None
        
        for attempt in range(self.retry_times):
            try:
                print(f"ğŸ” è·å–æ–‡ç« å†…å®¹ (å°è¯• {attempt + 1}/{self.retry_times}): {url[:50]}...")
                
                # å‘é€è¯·æ±‚
                response = requests.get(url, headers=self.headers, timeout=self.timeout)
                response.raise_for_status()
                
                # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°é”™è¯¯é¡µé¢
                if 'è¯¥å†…å®¹å·²è¢«å‘å¸ƒè€…åˆ é™¤' in response.text or 'æ­¤å†…å®¹å› è¿è§„æ— æ³•æŸ¥çœ‹' in response.text:
                    print(f"âš ï¸ æ–‡ç« å·²è¢«åˆ é™¤æˆ–è¿è§„")
                    return None
                
                # è§£æHTMLå†…å®¹
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æå–æ–‡ç« å†…å®¹
                content_data = self._extract_content(soup)
                
                if content_data and content_data.get('content'):
                    print(f"âœ… æˆåŠŸè·å–æ–‡ç« å†…å®¹: {len(content_data['content'])} å­—ç¬¦")
                    return content_data
                else:
                    print(f"âš ï¸ æœªèƒ½æå–åˆ°æ–‡ç« å†…å®¹")
                    
            except requests.exceptions.Timeout:
                print(f"â° è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{self.retry_times})")
            except requests.exceptions.RequestException as e:
                print(f"âŒ è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.retry_times}): {e}")
            except Exception as e:
                print(f"âŒ è§£æå¤±è´¥ (å°è¯• {attempt + 1}/{self.retry_times}): {e}")
            
            # é‡è¯•å‰ç­‰å¾…
            if attempt < self.retry_times - 1:
                time.sleep(2)
        
        print(f"âŒ è·å–æ–‡ç« å†…å®¹å¤±è´¥ï¼Œå·²é‡è¯• {self.retry_times} æ¬¡")
        return None
    
    def _is_wechat_article_url(self, url: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æ˜¯å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥
        
        Args:
            url: é“¾æ¥åœ°å€
            
        Returns:
            æ˜¯å¦æ˜¯å¾®ä¿¡æ–‡ç« é“¾æ¥
        """
        wechat_patterns = [
            r'mp\.weixin\.qq\.com/s/',
            r'mp\.weixin\.qq\.com/s\?',
        ]
        
        return any(re.search(pattern, url) for pattern in wechat_patterns)
    
    def _extract_content(self, soup: BeautifulSoup) -> Optional[Dict[str, str]]:
        """
        ä»BeautifulSoupå¯¹è±¡ä¸­æå–æ–‡ç« å†…å®¹
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            åŒ…å«æ ‡é¢˜å’Œå†…å®¹çš„å­—å…¸
        """
        try:
            # æå–æ ‡é¢˜
            title = self._extract_title(soup)
            
            # æå–æ­£æ–‡å†…å®¹
            content = self._extract_main_content(soup)
            
            if not content:
                return None
            
            return {
                'title': title or 'æ— æ ‡é¢˜',
                'content': content,
                'length': len(content)
            }
            
        except Exception as e:
            print(f"âŒ å†…å®¹æå–å¤±è´¥: {e}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """æå–æ–‡ç« æ ‡é¢˜"""
        # å°è¯•å¤šç§æ ‡é¢˜é€‰æ‹©å™¨
        title_selectors = [
            '#activity-name',
            '.rich_media_title',
            'h1.rich_media_title',
            'h2.rich_media_title',
            '.wx_article_title',
            'title'
        ]
        
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                if title and len(title) > 3:  # è¿‡æ»¤å¤ªçŸ­çš„æ ‡é¢˜
                    return title
        
        return None
    
    def _extract_main_content(self, soup: BeautifulSoup) -> Optional[str]:
        """æå–æ–‡ç« æ­£æ–‡å†…å®¹"""
        # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
        content_selectors = [
            '#js_content',
            '.rich_media_content',
            '.wx_article_content',
            '#img-content',
            '.article_content'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # æ¸…ç†å†…å®¹
                content = self._clean_content(content_elem)
                if content and len(content) > 100:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                    return content
        
        return None
    
    def _clean_content(self, content_elem) -> str:
        """
        æ¸…ç†æ–‡ç« å†…å®¹
        
        Args:
            content_elem: å†…å®¹å…ƒç´ 
            
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬å†…å®¹
        """
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for elem in content_elem.find_all(['script', 'style', 'noscript']):
            elem.decompose()
        
        # ç§»é™¤å¹¿å‘Šå’Œæ¨å¹¿ç›¸å…³çš„å…ƒç´ 
        ad_classes = ['ad', 'advertisement', 'promotion', 'sponsor']
        for ad_class in ad_classes:
            for elem in content_elem.find_all(class_=re.compile(ad_class, re.I)):
                elem.decompose()
        
        # è·å–æ–‡æœ¬å†…å®¹
        text = content_elem.get_text(separator='\n', strip=True)
        
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'\n\s*\n', '\n\n', text)  # åˆå¹¶å¤šä¸ªç©ºè¡Œ
        text = re.sub(r'[ \t]+', ' ', text)      # åˆå¹¶å¤šä¸ªç©ºæ ¼
        text = text.strip()
        
        return text
    
    def batch_fetch_contents(self, articles: list) -> list:
        """
        æ‰¹é‡è·å–æ–‡ç« å†…å®¹
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡ç« åº”åŒ…å«'link'å­—æ®µ
            
        Returns:
            æ›´æ–°åçš„æ–‡ç« åˆ—è¡¨ï¼Œæ·»åŠ äº†'full_content'å­—æ®µ
        """
        updated_articles = []
        
        for i, article in enumerate(articles, 1):
            print(f"\nğŸ“– å¤„ç†æ–‡ç«  {i}/{len(articles)}")
            
            link = article.get('link', '')
            if not link:
                print(f"âš ï¸ æ–‡ç« ç¼ºå°‘é“¾æ¥")
                updated_articles.append(article)
                continue
            
            # è·å–å®Œæ•´å†…å®¹
            content_data = self.fetch_article_content(link)
            
            if content_data:
                article['full_content'] = content_data['content']
                article['content_length'] = content_data['length']
                print(f"âœ… è·å–æˆåŠŸ: {content_data['length']} å­—ç¬¦")
            else:
                article['full_content'] = article.get('summary', '') or article.get('title', '')
                article['content_length'] = len(article['full_content'])
                print(f"âš ï¸ ä½¿ç”¨å¤‡ç”¨å†…å®¹: {article['content_length']} å­—ç¬¦")
            
            updated_articles.append(article)
            
            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            if i < len(articles):
                time.sleep(1)
        
        return updated_articles


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    fetcher = WeChatContentFetcher()
    
    # æµ‹è¯•æ–‡ç« é“¾æ¥
    test_url = "https://mp.weixin.qq.com/s/test_article_id"
    
    content = fetcher.fetch_article_content(test_url)
    if content:
        print(f"æ ‡é¢˜: {content['title']}")
        print(f"å†…å®¹é•¿åº¦: {content['length']} å­—ç¬¦")
        print(f"å†…å®¹é¢„è§ˆ: {content['content'][:200]}...")
    else:
        print("è·å–å†…å®¹å¤±è´¥")
