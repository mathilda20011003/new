#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å·¥å…·å‡½æ•°æ¨¡å—
"""

import os
import yaml
from typing import List, Dict, Any


def load_config(config_path: str) -> Dict[str, Any]:
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    
    Returns:
        é…ç½®å­—å…¸
    """
    try:
        # æ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„
        if not os.path.isabs(config_path):
            # è·å–é¡¹ç›®æ ¹ç›®å½•
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(current_dir)
            config_path = os.path.join(project_root, config_path)
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        return config
        
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        raise
    except yaml.YAMLError as e:
        print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        raise


def filter_by_keywords(articles: List[Dict], keywords: List[str]) -> List[Dict]:
    """
    æ ¹æ®å…³é”®è¯ç­›é€‰æ–‡ç« 
    
    Args:
        articles: æ–‡ç« åˆ—è¡¨
        keywords: å…³é”®è¯åˆ—è¡¨
    
    Returns:
        ç­›é€‰åçš„æ–‡ç« åˆ—è¡¨
    """
    if not keywords:
        return articles
    
    filtered = []
    
    for article in articles:
        title = article.get('title', '').lower()
        summary = article.get('summary', '').lower()
        content = article.get('content', '').lower()
        
        # ç»„åˆæ‰€æœ‰æ–‡æœ¬
        text = f"{title} {summary} {content}"
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»æ„å…³é”®è¯
        for keyword in keywords:
            if keyword.lower() in text:
                filtered.append(article)
                break
    
    return filtered


def format_article_message(article: Dict) -> str:
    """
    æ ¼å¼åŒ–æ–‡ç« æ¶ˆæ¯
    
    Args:
        article: æ–‡ç« å­—å…¸
    
    Returns:
        æ ¼å¼åŒ–åçš„æ¶ˆæ¯
    """
    title = article.get('title', 'æ— æ ‡é¢˜')
    link = article.get('link', '')
    ai_summary = article.get('ai_summary', 'æš‚æ— æ‘˜è¦')
    account_name = article.get('account_name', 'æœªçŸ¥å…¬ä¼—å·')
    published = article.get('published', '')
    
    message_parts = [
        f"ğŸ“° **{title}**",
        f"ğŸ“± æ¥æº: {account_name}",
        f"ğŸ“ æ‘˜è¦: {ai_summary}",
    ]
    
    if published:
        message_parts.append(f"â° å‘å¸ƒ: {published}")
    
    if link:
        message_parts.append(f"ğŸ”— é“¾æ¥: {link}")
    
    return "\n".join(message_parts)


def truncate_text(text: str, max_length: int = 100, suffix: str = "...") -> str:
    """
    æˆªæ–­æ–‡æœ¬
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        max_length: æœ€å¤§é•¿åº¦
        suffix: åç¼€
    
    Returns:
        æˆªæ–­åçš„æ–‡æœ¬
    """
    if len(text) <= max_length:
        return text
    return text[:max_length] + suffix


def load_frequency_words(file_path: str = "config/frequency_words.txt") -> List[str]:
    """
    åŠ è½½å…³é”®è¯æ–‡ä»¶ï¼ˆå¤ç”¨ç°æœ‰çš„ frequency_words.txtï¼‰
    
    Args:
        file_path: å…³é”®è¯æ–‡ä»¶è·¯å¾„
    
    Returns:
        å…³é”®è¯åˆ—è¡¨
    """
    try:
        # æ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„
        if not os.path.isabs(file_path):
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(current_dir)
            file_path = os.path.join(project_root, file_path)
        
        keywords = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                if not line or line.startswith('#'):
                    continue
                # è·³è¿‡è¿‡æ»¤è¯ï¼ˆ!å¼€å¤´ï¼‰
                if line.startswith('!'):
                    continue
                # ç§»é™¤å¿…é¡»è¯æ ‡è®°ï¼ˆ+å¼€å¤´ï¼‰
                if line.startswith('+'):
                    line = line[1:]
                
                keywords.append(line)
        
        return keywords
        
    except FileNotFoundError:
        print(f"âš ï¸  å…³é”®è¯æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return []


def save_articles_to_json(articles: List[Dict], output_path: str = "data/wechat_articles.json"):
    """
    ä¿å­˜æ–‡ç« åˆ° JSON æ–‡ä»¶
    
    Args:
        articles: æ–‡ç« åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    import json
    from datetime import datetime
    
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # æ·»åŠ æ—¶é—´æˆ³
        data = {
            'timestamp': datetime.now().isoformat(),
            'count': len(articles),
            'articles': articles
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ–‡ç« å·²ä¿å­˜åˆ°: {output_path}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ç« å¤±è´¥: {e}")


def load_articles_from_json(input_path: str = "data/wechat_articles.json") -> List[Dict]:
    """
    ä» JSON æ–‡ä»¶åŠ è½½æ–‡ç« 
    
    Args:
        input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
    
    Returns:
        æ–‡ç« åˆ—è¡¨
    """
    import json
    
    try:
        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        return data.get('articles', [])
        
    except FileNotFoundError:
        print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return []
    except json.JSONDecodeError as e:
        print(f"âŒ JSON è§£æå¤±è´¥: {e}")
        return []


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # æµ‹è¯•åŠ è½½é…ç½®
    try:
        config = load_config("config/wechat_accounts.yaml")
        print("âœ… é…ç½®åŠ è½½æˆåŠŸ:")
        print(f"   å…¬ä¼—å·æ•°é‡: {len(config.get('accounts', []))}")
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
    
    # æµ‹è¯•å…³é”®è¯ç­›é€‰
    test_articles = [
        {'title': 'OpenAI å‘å¸ƒ Sora', 'summary': 'AI è§†é¢‘ç”Ÿæˆ'},
        {'title': 'ä»Šå¤©å¤©æ°”ä¸é”™', 'summary': 'é˜³å…‰æ˜åªš'},
        {'title': 'Midjourney V7', 'summary': 'AI ç»˜ç”»å·¥å…·'}
    ]
    
    keywords = ['AI', 'Sora', 'Midjourney']
    filtered = filter_by_keywords(test_articles, keywords)
    print(f"\nâœ… å…³é”®è¯ç­›é€‰æµ‹è¯•:")
    print(f"   åŸå§‹: {len(test_articles)} ç¯‡")
    print(f"   ç­›é€‰å: {len(filtered)} ç¯‡")
    
    # æµ‹è¯•åŠ è½½å…³é”®è¯æ–‡ä»¶
    frequency_words = load_frequency_words()
    if frequency_words:
        print(f"\nâœ… å…³é”®è¯æ–‡ä»¶åŠ è½½æˆåŠŸ:")
        print(f"   å…³é”®è¯æ•°é‡: {len(frequency_words)}")
        print(f"   å‰ 5 ä¸ª: {frequency_words[:5]}")

